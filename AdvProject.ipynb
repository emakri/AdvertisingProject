{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem Statement\n",
    "This is a problem that companies often have while running their marketing campaigns. The dataset is attached below. The task is to predict which customers will most likely click on the ad. Let’s consider that you are working for a marketing company. We assume that you have an online marketing campaign for which you spend \\\\$1000 per potential customer. For each customer that you target with your digital ad campaign and that clicks on the ad, let’s assume that you’ll get a net profit of $100. Your task is to come up with a model that will maximize the profit of the company. \n",
    "- How you would answer this problem? \n",
    "- Are there any features more important than others? \n",
    "- Can you train a model that will make adequate prediction. \n",
    "- Do you need to perform feature engineering\n",
    "- Do you need to do any preprocessing (data cleansing, imputation, etc)\n",
    "- Can you justify each process that you performed on the dataset?\n",
    "- What would be your recommendations for the company?\n",
    "- What would be the next step?The dataset is fairly straight forward to understand so if you are experienced, you might feel less challenged. If you are a beginner, feel free to start easy with a simple approach. Take the time to do exploratory data analysis to fully understand the dataset. Make sure that you spend some time understanding the relationship between the different features. Feel free to ask questions, and try it out on your own first as if it was a take-home assignment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Exploration\n",
    "\n",
    "We will first import the data and take a first look. We will also try to identify and address obvious issues with the data, such as missing values, duplicates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('advertising_dsdj.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Daily Time Spent on Site</th>\n",
       "      <th>Age</th>\n",
       "      <th>Area Income</th>\n",
       "      <th>Daily Internet Usage</th>\n",
       "      <th>Ad Topic Line</th>\n",
       "      <th>City</th>\n",
       "      <th>Male</th>\n",
       "      <th>Country</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Clicked on Ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.95</td>\n",
       "      <td>35</td>\n",
       "      <td>61833.90</td>\n",
       "      <td>256.09</td>\n",
       "      <td>Cloned 5thgeneration orchestration</td>\n",
       "      <td>Wrightburgh</td>\n",
       "      <td>0</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>2016-03-27 0:53</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80.23</td>\n",
       "      <td>31</td>\n",
       "      <td>68441.85</td>\n",
       "      <td>193.77</td>\n",
       "      <td>Monitored national standardization</td>\n",
       "      <td>West Jodi</td>\n",
       "      <td>1</td>\n",
       "      <td>Nauru</td>\n",
       "      <td>2016-04-04 1:39</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.47</td>\n",
       "      <td>26</td>\n",
       "      <td>59785.94</td>\n",
       "      <td>236.50</td>\n",
       "      <td>Organic bottom-line service-desk</td>\n",
       "      <td>Davidton</td>\n",
       "      <td>0</td>\n",
       "      <td>San Marino</td>\n",
       "      <td>2016-03-13 20:35</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>74.15</td>\n",
       "      <td>29</td>\n",
       "      <td>54806.18</td>\n",
       "      <td>245.89</td>\n",
       "      <td>Triple-buffered reciprocal time-frame</td>\n",
       "      <td>West Terrifurt</td>\n",
       "      <td>1</td>\n",
       "      <td>Italy</td>\n",
       "      <td>2016-01-10 2:31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>68.37</td>\n",
       "      <td>35</td>\n",
       "      <td>73889.99</td>\n",
       "      <td>225.58</td>\n",
       "      <td>Robust logistical utilization</td>\n",
       "      <td>South Manuel</td>\n",
       "      <td>0</td>\n",
       "      <td>Iceland</td>\n",
       "      <td>2016-06-03 3:36</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Daily Time Spent on Site  Age  Area Income  Daily Internet Usage  \\\n",
       "0                     68.95   35     61833.90                256.09   \n",
       "1                     80.23   31     68441.85                193.77   \n",
       "2                     69.47   26     59785.94                236.50   \n",
       "3                     74.15   29     54806.18                245.89   \n",
       "4                     68.37   35     73889.99                225.58   \n",
       "\n",
       "                           Ad Topic Line            City  Male     Country  \\\n",
       "0     Cloned 5thgeneration orchestration     Wrightburgh     0     Tunisia   \n",
       "1     Monitored national standardization       West Jodi     1       Nauru   \n",
       "2       Organic bottom-line service-desk        Davidton     0  San Marino   \n",
       "3  Triple-buffered reciprocal time-frame  West Terrifurt     1       Italy   \n",
       "4          Robust logistical utilization    South Manuel     0     Iceland   \n",
       "\n",
       "          Timestamp  Clicked on Ad  \n",
       "0   2016-03-27 0:53            0.0  \n",
       "1   2016-04-04 1:39            0.0  \n",
       "2  2016-03-13 20:35            0.0  \n",
       "3   2016-01-10 2:31            0.0  \n",
       "4   2016-06-03 3:36            0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1018 entries, 0 to 1017\n",
      "Data columns (total 10 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Daily Time Spent on Site  1018 non-null   float64\n",
      " 1   Age                       1018 non-null   int64  \n",
      " 2   Area Income               1018 non-null   float64\n",
      " 3   Daily Internet Usage      1018 non-null   float64\n",
      " 4   Ad Topic Line             1018 non-null   object \n",
      " 5   City                      1018 non-null   object \n",
      " 6   Male                      1018 non-null   int64  \n",
      " 7   Country                   1018 non-null   object \n",
      " 8   Timestamp                 1018 non-null   object \n",
      " 9   Clicked on Ad             1014 non-null   float64\n",
      "dtypes: float64(4), int64(2), object(4)\n",
      "memory usage: 79.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Daily Time Spent on Site    0\n",
       "Age                         0\n",
       "Area Income                 0\n",
       "Daily Internet Usage        0\n",
       "Ad Topic Line               0\n",
       "City                        0\n",
       "Male                        0\n",
       "Country                     0\n",
       "Timestamp                   0\n",
       "Clicked on Ad               4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "data.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 4 missing values in the target variable so we will drop these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "print('Number of duplicated records in dataset:', data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the duplicated records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory data analysis\n",
    "\n",
    "## 4.1 Describing the features and data cleaning\n",
    "\n",
    "First, let's inspect the target variable for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect for class imbalance of the target variable\n",
    "data['Clicked on Ad'].value_counts() / data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very well balanced classification problem. Therefore, we don't have to worry about class imbalance issues.\n",
    "\n",
    "Let us separate numerical and categorical variables to take a quick look at the summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_cat_columns(df):\n",
    "    '''\n",
    "    Separates numerical and categorical (type object) variables in given dataframe.\n",
    "    Returns a 2 element list containing the list of numerical variable names as first \n",
    "    element and the list of categorical variable names as second element.\n",
    "    This is a preliminary separation based on data type upon import and should be used\n",
    "    with caution, as some int64 variables may be randomly generated IDs and not true \n",
    "    numerical variables and some categorical variables might be represented as numerical\n",
    "    (e.g. classes).\n",
    "    '''\n",
    "    col_num = []\n",
    "    col_cat = []\n",
    "    for column in df.columns:\n",
    "        if df[str(column)].dtypes != 'O':       \n",
    "            col_num.append(column)\n",
    "        else:\n",
    "            col_cat.append(column)\n",
    "    return [col_num, col_cat]\n",
    "\n",
    "[col_num, col_cat] = num_cat_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_num].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_cat].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for most of the true numerical features (not the classes 'Male' and 'Clicked on Ad'), the mean and the median are relatively close, indicating low skewness. This suggests that we are not likely to have to transform the data based on skewness when engineering features. We can confirm this later after visualisations.\n",
    "\n",
    "However, we see that there are some bizarre values for the age feature: the minimum is negative and the maximum corresponds to an unrealistic human age. Let's visualise the Age feature by sorting it and plotting it against its index to easily see the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_age = sorted(data['Age'])\n",
    "indices = []\n",
    "for i in range(len(sorted_age)):\n",
    "    indices.append(i)\n",
    "x = indices\n",
    "y = sorted_age\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(x, y)\n",
    "plt.axhline(y=0, linestyle='dotted', color='r')\n",
    "plt.axhline(y=100, linestyle='dotted', color='r')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the observations we have for extreme age values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['Age'] < 18) | (data['Age'] > 80) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop these observations as they are either obviously wrong or very likely to be included in the dataset due to error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['Age'] >= 18) & (data['Age'] <= 80) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will transfrom the Timestamp feature to datetime to be able to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also confirm that the time spent on site is less than the daily internet usage for all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[(data['Daily Internet Usage'] - data['Daily Time Spent on Site'] < 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally remove these observations too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(data['Daily Internet Usage'] - data['Daily Time Spent on Site'] < 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data Visualisation\n",
    "\n",
    "First, we will define a function to use for univariate data visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univar_plot_num(df, col, target):\n",
    "    '''\n",
    "    Function to plot numerical features. Creates two subplots: a histogram and a \n",
    "    boxplot where the observations of the column are separating according to the\n",
    "    target.\n",
    "    '''\n",
    "    df_clean = df.dropna()\n",
    "    f, axes = plt.subplots(ncols = 2, figsize = (14, 6))\n",
    "    plt.suptitle('Feature: ' + col)\n",
    "    sns.histplot(df_clean[col], kde = True, ax = axes[0])\n",
    "    sns.boxplot(y = col, x = target, data = df_clean, ax = axes[1])\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a univariate plot of a histogram of the numerical variables and a boxplot where the data is split according to the value of the target varibale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_num_plot = col_num[:]\n",
    "del(cols_num_plot[-1])\n",
    "cols_num_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_num_plot:\n",
    "    univar_plot_num(data, col, 'Clicked on Ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Daily Time Spent on Site\" and the \"Daily Internet Usage\" appear to be slightly bimodal. This could suggest the existence of different groups of customers. \n",
    "\n",
    "In addition, there appears to be some skewness for the \"Age\" and \"Area Income\" columns, indicating that we could try a logarithmic transformation for the right-skewed age feature when modeling our data to avoid bias.\n",
    "\n",
    "We can see from the box plots that \"Daily Time Spent on Site\" and \"Daily Internet Usage\" might have significant predictive power based on the difference between the groups of people who clicked on the ad vs. did not click. The \"Age\" and \"Area Income\" also could potentially be useful.\n",
    "\n",
    "Let's look into correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot(df):\n",
    "    correlation = df.corr()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(correlation, \n",
    "          xticklabels=correlation.columns.values,\n",
    "          yticklabels=correlation.columns.values)\n",
    "    print(correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_plot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of gender, we observe some significant correlation between all features and the target. However, there seems to be high correlation between some of the predictive features, which is a fact that we might have to revisit later.\n",
    "\n",
    "It is now time to inspect the categorical variables. We will call .describe() again to see how things look like after data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[col_cat].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that \"Ad Topic Line\" and \"City\" probably have no predictive power, but we can take a look at the country feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicked_country = pd.crosstab(data['Country'], data['Clicked on Ad'], rownames=['Country'], colnames=['Clicked On Ad'])\n",
    "clicked_country.sort_values(1, ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like there is a big spread across 237 countries, with a maximum of 9 clicks per countr, therefore the distribution will not be interesting to look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's do some pair plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.pairplot(data=data[col_num],\n",
    "             hue=\"Clicked on Ad\",\n",
    "             dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Feature selection and engineering\n",
    "We will proceed with some feature selection and engineering, as well as some preprocessing in order to be able to use the data to train a classification algorithm.\n",
    "### 4.3.1 Dropping features\n",
    "\n",
    "To simplify our model, we will drop some features that do not seem to offer predictive value as described above or that would require special considerations, such as time analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(['City', 'Ad Topic Line', 'Timestamp', 'City'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 Tranformations for skewed features\n",
    "\n",
    "Earlier, the \"Age\" feature was observed to be right-skewed. Therefore, we will perform a logarithmic transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_age'] = np.log(df['Age'])\n",
    "df = df.drop('Age', axis = 1)\n",
    "univar_plot_num(df, 'log_age', 'Clicked on Ad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skewness of the distribution is visibly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 Scaling of numerical variables\n",
    "We will scale the numerical variables to acheive better results with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_variables = ['Country']\n",
    "numer_variables = ['Daily Time Spent on Site', 'Area Income', 'Daily Internet Usage', 'Male', 'log_age']\n",
    "target = 'Clicked on Ad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df[numer_variables] = scaler.fit_transform(df[numer_variables])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.4 One Hot Encoding for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelling\n",
    "In this section, we will attempt a few solutions and then select the best model. We will use F1 score as the performance metric.\n",
    "## 5.1 Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining training and test set \n",
    "X, y = df_model.drop(target,1).values, df[target].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=476,\n",
    "                                                  stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model selection\n",
    "## 5.2.1 Logistic regression \n",
    "We will start with a quick and dirty implementation of a logistic regression with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
